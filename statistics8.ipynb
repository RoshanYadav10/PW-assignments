{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. Explain the assumptions required to use ANOVA and provide examples of violations that could impact\n",
    "# the validity of the results.\n",
    "\n",
    "# Analysis of Variance (ANOVA) is a powerful statistical technique used to compare means between multiple groups. However, ANOVA comes with certain\n",
    "#  assumptions that need to be met in order to ensure the validity and reliability of the results. Violating these assumptions can lead to incorrect\n",
    "#   conclusions and interpretations. Here are the key assumptions of ANOVA:\n",
    "\n",
    "# 1. Independence of Observations:\n",
    "# The observations within each group should be independent of each other. This means that the values in one group should not be influenced by or \n",
    "# related to the values in another group.\n",
    "\n",
    "# 2. Normality:\n",
    "# The distribution of the residuals (the differences between individual data points and their respective group means) within each group should be \n",
    "# approximately normally distributed. This assumption is important because ANOVA relies on the normality of the residuals for accurate results.\n",
    "\n",
    "# 3. Homogeneity of Variances (Homoscedasticity):\n",
    "# The variability (or spread) of the residuals should be roughly equal across all groups. This is known as homogeneity of variances. Violating this \n",
    "# assumption can lead to unequal influence of groups on the overall analysis.\n",
    "\n",
    "# 4. Equal Sample Sizes (in some cases):\n",
    "# While ANOVA is robust to unequal sample sizes, equal sample sizes are preferred as they can improve the power and sensitivity of the test. If \n",
    "# sample sizes are vastly different, it's important to check if the results are consistent with the assumption.\n",
    "\n",
    "# Now, let's look at examples of violations for each assumption:\n",
    "\n",
    "# 1. Independence of Observations:\n",
    "# Violation Example: In a study comparing test scores of students from different schools, if students within the same school are given similar \n",
    "# preparation, the scores within each school might be correlated, violating independence.\n",
    "\n",
    "# 2. Normality:\n",
    "# Violation Example: In a study comparing reaction times of three different age groups, if the data is heavily skewed or has extreme outliers, the \n",
    "# normality assumption might be violated.\n",
    "\n",
    "# 3. Homogeneity of Variances:\n",
    "# Violation Example: In a study comparing the yields of different types of crops, if the variance of crop yields in one group is much larger than \n",
    "# in another group, the assumption of equal variance might be violated.\n",
    "\n",
    "# 4. Equal Sample Sizes:\n",
    "# Violation Example: In a study comparing the effectiveness of three different teaching methods, if one method has a significantly larger number \n",
    "# of students compared to the other methods, the assumption of equal sample sizes might be violated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. What are the three types of ANOVA, and in what situations would each be used?\n",
    "# Types of ANOVA:-\n",
    "\n",
    "# 1.One way ANOVA-One factor with atleast 2 levels,these levels are independent\n",
    "\n",
    "# 2.Repeated Measures Anova-One factor with atleast 2 levels,these levels are dependent\n",
    "\n",
    "# 3.Factorial ANOVA-Two or more factors (each with atleast 2 level) levels can be either independent and dependent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. What is the partitioning of variance in ANOVA, and why is it important to understand this concept?\n",
    "\n",
    "# The partitioning of variance in ANOVA refers to the breakdown of the total variability observed in the data into different components that can be\n",
    "# attributed to specific sources or factors. \n",
    "\n",
    "# In ANOVA, the total variability is divided into two main components:\n",
    "\n",
    "# 1. Between-Group Variability: This component represents the variation in the dependent variable that is due to differences between the various \n",
    "# groups or conditions being compared. In other words, it measures how much the group means differ from each other. This is also known as \n",
    "# the \"treatment effect\" or \"factor effect.\"\n",
    "\n",
    "# 2. Within-Group Variability (or Residual Variability): This component represents the variation within each group or condition. It measures the \n",
    "# variation of individual data points around their respective group means. It represents the variability that cannot be explained by the factors \n",
    "# being studied.\n",
    "\n",
    "# The total variability in the data can then be mathematically decomposed into these two components:\n",
    "\n",
    "# Total Variability = Between-Group Variability + Within-Group Variability\n",
    "\n",
    "# Understanding the partitioning of variance is important for several reasons:\n",
    "\n",
    "# 1. Identifying Sources of Variation:By partitioning the total variance, ANOVA allows you to attribute the observed differences in means to \n",
    "# different factors. This helps in understanding which factors are contributing to the observed effects.\n",
    "\n",
    "# 2. Assessing Significance:ANOVA helps assess whether the between-group variability is statistically significant compared to the within-group \n",
    "# variability. If the between-group variability is large relative to the within-group variability, it suggests that the factor being studied has \n",
    "# a significant effect.\n",
    "\n",
    "# 3. Interpreting Results: The partitioning of variance provides a clear framework for interpreting ANOVA results. It helps you quantify the \n",
    "# relative impact of different factors on the outcome variable.\n",
    "\n",
    "# 4. Study Design and Improvement: Understanding how different factors contribute to variability can guide future research and experimental\n",
    "#  design. It can help identify which factors might be worth investigating further or controlling for in future studies.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sum of Squares (SST): 1152.9333333333332\n",
      "Explained Sum of Squares (SSE): 1040.9333333333338\n",
      "Residual Sum of Squares (SSR): 111.99999999999932\n"
     ]
    }
   ],
   "source": [
    "# Q4. How would you calculate the total sum of squares (SST), explained sum of squares (SSE), and residual\n",
    "# sum of squares (SSR) in a one-way ANOVA using Python?\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Example data for three groups\n",
    "group_a = np.array([68, 72, 75, 70, 74])\n",
    "group_b = np.array([80, 85, 78, 82, 87])\n",
    "group_c = np.array([60, 62, 65, 59, 64])\n",
    "\n",
    "# Combine all data into a single array\n",
    "all_data = np.concatenate((group_a, group_b, group_c))\n",
    "\n",
    "# Calculate overall mean\n",
    "overall_mean = np.mean(all_data)\n",
    "\n",
    "# Calculate Total Sum of Squares (SST)\n",
    "sst = np.sum((all_data - overall_mean)**2)\n",
    "\n",
    "# Calculate group means\n",
    "mean_a = np.mean(group_a)\n",
    "mean_b = np.mean(group_b)\n",
    "mean_c = np.mean(group_c)\n",
    "\n",
    "# Calculate Explained Sum of Squares (SSE)\n",
    "sse = (len(group_a) * (mean_a - overall_mean)**2 +\n",
    "       len(group_b) * (mean_b - overall_mean)**2 +\n",
    "       len(group_c) * (mean_c - overall_mean)**2)\n",
    "\n",
    "# Calculate Residual Sum of Squares (SSR)\n",
    "ssr = sst - sse\n",
    "\n",
    "print(\"Total Sum of Squares (SST):\", sst)\n",
    "print(\"Explained Sum of Squares (SSE):\", sse)\n",
    "print(\"Residual Sum of Squares (SSR):\", ssr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  sum_sq   df           F    PR(>F)\n",
      "FactorA          190.125  1.0  217.285714  0.000123\n",
      "FactorB           10.125  1.0   11.571429  0.027235\n",
      "FactorA:FactorB    0.125  1.0    0.142857  0.724659\n",
      "Residual           3.500  4.0         NaN       NaN\n"
     ]
    }
   ],
   "source": [
    "# Q5. In a two-way ANOVA, how would you calculate the main effects and interaction effects using Python?\n",
    "\n",
    "# In a two-way ANOVA, you can calculate the main effects of each independent variable (factor) and the interaction effect between the two \n",
    "# independent variables. The main effects represent the influence of each factor on the dependent variable, while the interaction effect examines \n",
    "# whether the combined influence of the factors is different from what would be expected based on their individual effects. \n",
    "\n",
    "# Here's how you can calculate main effects and interaction effects using Python:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Example data for two-way ANOVA\n",
    "data = {'FactorA': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],\n",
    "        'FactorB': ['X', 'Y', 'X', 'Y', 'X', 'Y', 'X', 'Y'],\n",
    "        'Values': [10, 12, 9, 11, 20, 22, 18, 21]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Perform two-way ANOVA\n",
    "model = ols('Values ~ FactorA * FactorB', data=df).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "print(anova_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Q6. Suppose you conducted a one-way ANOVA and obtained an F-statistic of 5.23 and a p-value of 0.02.\n",
    "# # What can you conclude about the differences between the groups, and how would you interpret these\n",
    "# # results?\n",
    "\n",
    "# In a one-way ANOVA, the F-statistic and the associated p-value are used to determine whether there are statistically significant differences \n",
    "# among the means of the groups being compared. Let's interpret the results you provided:\n",
    "\n",
    "# F-Statistic: 5.23\n",
    "# The F-statistic is a measure of the ratio of variability between group means to variability within groups. In your case, the F-statistic is 5.23.\n",
    "\n",
    "# P-Value: 0.02\n",
    "# The p-value indicates the probability of observing the obtained F-statistic (or a more extreme value) if the null hypothesis is true. In your \n",
    "# case, the p-value is 0.02.\n",
    "\n",
    "# Interpretation:\n",
    "\n",
    "# Since the p-value (0.02) is less than the typical significance level of 0.05 (or 5%), we can conclude the following:\n",
    "\n",
    "# Reject the Null Hypothesis: The null hypothesis in this case states that there are no significant differences among the group means. Since the \n",
    "# p-value is below 0.05, we have enough evidence to reject the null hypothesis.\n",
    "\n",
    "# Conclude Significant Differences: With a low p-value, we can conclude that there are statistically significant differences among at least some \n",
    "# of the group means. In other words, there is evidence to suggest that the groups are not all the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Q7. In a repeated measures ANOVA, how would you handle missing data, and what are the potential\n",
    "# # consequences of using different methods to handle missing data?\n",
    "\n",
    "# Handling missing data in a repeated measures ANOVA is crucial to ensure the accuracy and reliability of your results. Missing data can arise for\n",
    "# various reasons, such as participant dropout, technical errors, or incomplete responses. There are several methods to handle missing data, each\n",
    "# with its own advantages and potential consequences. Here are some common methods and their potential consequences:\n",
    "\n",
    "# 1. Listwise Deletion (Complete Case Analysis):\n",
    "#    This involves excluding participants with missing data from the analysis. While it's straightforward, it can lead to loss of statistical \n",
    "#    power and potential bias if the missing data are not completely random (missing completely at random, MCAR).\n",
    "\n",
    "#    Consequence: Reduced sample size, biased results if missing data are not MCAR.\n",
    "\n",
    "# 2. Mean Imputation:\n",
    "#    Missing values are replaced with the mean value of the observed data for that variable. It's simple but can underestimate the variability \n",
    "#    and distort relationships.\n",
    "\n",
    "#    Consequence: Underestimation of variability, biased results, reduced statistical power.\n",
    "\n",
    "# 3. Last Observation Carried Forward (LOCF) or Next Observation Carried Backward (NOCB):\n",
    "#    Missing data are replaced with the last observed value (LOCF) or the next observed value (NOCB). These methods assume that the missing value \n",
    "#    is similar to the last or next observed value, which may not be accurate.\n",
    "\n",
    "#    Consequence:Distortion of data patterns, may not accurately reflect changes over time.\n",
    "\n",
    "# 4. Linear Interpolation:\n",
    "#    Missing values are estimated based on linear interpolation between adjacent observed values. This method assumes a linear relationship between \n",
    "#    data points and may not be appropriate for non-linear data.\n",
    "\n",
    "#    Consequence: May introduce artificial trends or patterns, particularly in non-linear data.\n",
    "\n",
    "# 5. Multiple Imputation:\n",
    "#    Multiple imputation generates multiple datasets, each with different imputed values. These datasets are analyzed separately, and the results \n",
    "#    are combined to account for uncertainty due to missing data. This method is statistically rigorous but computationally intensive.\n",
    "\n",
    "#    Consequence: Requires more computational resources, complexity in implementation.\n",
    "\n",
    "# 6. Model-Based Imputation:\n",
    "#    Impute missing data using a predictive model based on other variables. This can be more accurate if relationships among variables are \n",
    "#    well-understood.\n",
    "\n",
    "#    Consequence: Relies on the chosen model's accuracy, risk of propagating model errors.\n",
    "\n",
    "# 7. Missing Data Indicator Variable:\n",
    "#    Include a binary variable indicating whether data is missing. This allows the ANOVA to treat missing data as a separate group. This can be \n",
    "#    informative if missingness is related to a specific factor.\n",
    "\n",
    "#    Consequence: Adds complexity to analysis, requires additional assumptions about missing data mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8. What are some common post-hoc tests used after ANOVA, and when would you use each one? Provide\n",
    "# an example of a situation where a post-hoc test might be necessary.\n",
    "\n",
    "# After conducting an Analysis of Variance (ANOVA) and finding a significant difference among group means, post-hoc tests are used to determine \n",
    "# which specific groups are significantly different from each other. These tests help avoid the problem of inflated Type I error rates that can \n",
    "# occur when conducting multiple pairwise comparisons. Here are some common post-hoc tests and situations where they might be used:\n",
    "\n",
    "# 1. Tukey's Honestly Significant Difference (HSD):\n",
    "#    Tukey's HSD test is widely used and compares all possible pairs of group means. It controls the familywise error rate, making it suitable \n",
    "#    when you're comparing multiple groups and want to maintain an overall Type I error rate.\n",
    "\n",
    "#    Example:In a study comparing the effectiveness of three different treatments on pain relief, after conducting an ANOVA and finding a \n",
    "#    significant difference, you use Tukey's HSD to determine which specific pairs of treatments have significantly different effects.\n",
    "\n",
    "# 2. Bonferroni Correction:\n",
    "#    The Bonferroni correction involves adjusting the significance level (alpha) for each individual comparison to control the overall familywise \n",
    "#    error rate. It's conservative but helpful when you want to reduce the risk of false positives.\n",
    "\n",
    "#    Example: If you're comparing the preferences for multiple flavors of ice cream, and you don't want to risk falsely concluding that two \n",
    "#    flavors are different when they might not be, you can use Bonferroni correction for pairwise comparisons.\n",
    "\n",
    "# 3. Scheffe's Method:\n",
    "#    Scheffe's method is less sensitive to Type I errors but is suitable for situations where you have a small number of comparisons. It is robust \n",
    "#    and can be used when there are unequal sample sizes or unequal variances.\n",
    "\n",
    "#    Example:In an educational study where you're comparing the performance of students from different schools on multiple subjects, Scheffe's\n",
    "#     method could be used to compare schools while controlling the familywise error rate.\n",
    "\n",
    "# 4. Dunn's Test:\n",
    "#    Dunn's test, also known as the Dunn-Bonferroni test, is a non-parametric post-hoc test suitable for situations with unequal variances and \n",
    "#    non-normal distributions. It's less sensitive to distributional assumptions.\n",
    "\n",
    "#    Example: If you're comparing the performance of different software systems on various metrics and the data is not normally distributed, \n",
    "#    Dunn's test can be used to make pairwise comparisons.\n",
    "\n",
    "# 5. Games-Howell Test:\n",
    "#    The Games-Howell test is used when the assumption of equal variances is violated. It's more robust in such cases compared to other tests \n",
    "#    that assume equal variances.\n",
    "\n",
    "#    Example: Suppose you're comparing the reaction times of participants under different experimental conditions, and the variances are not \n",
    "#    equal. In this case, you can use the Games-Howell test for pairwise comparisons.\n",
    "\n",
    "# Remember that the choice of post-hoc test depends on factors like sample size, distribution of data, and assumptions being met. It's essential \n",
    "# to consider the characteristics of your data and the goals of your analysis to select an appropriate post-hoc test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-Statistic: 572.5521283395123\n",
      "P-Value: 4.1443235024109274e-70\n",
      "There are significant differences between the mean weight loss of the three diets.\n"
     ]
    }
   ],
   "source": [
    "# Q9. A researcher wants to compare the mean weight loss of three diets: A, B, and C. They collect data from\n",
    "# 50 participants who were randomly assigned to one of the diets. Conduct a one-way ANOVA using Python\n",
    "# to determine if there are any significant differences between the mean weight loss of the three diets.\n",
    "# Report the F-statistic and p-value, and interpret the results.\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Example data for weight loss of three diets (A, B, C)\n",
    "diet_a = np.array([3.5, 2.8, 4.2, 2.0, 3.9, 2.5, 2.7, 3.3, 3.8, 2.2,\n",
    "                   3.0, 3.7, 3.1, 3.6, 3.4, 2.9, 2.3, 3.8, 2.8, 3.2,\n",
    "                   2.5, 3.4, 2.7, 3.1, 2.9, 3.2, 2.8, 2.6, 3.7, 3.4,\n",
    "                   2.7, 2.9, 3.5, 2.3, 3.1, 2.8, 3.6, 2.4, 3.3, 2.6,\n",
    "                   3.0, 2.8, 3.5, 2.7, 2.9, 3.1, 3.3, 2.5, 2.8, 3.2])\n",
    "\n",
    "diet_b = np.array([2.1, 1.9, 1.8, 2.0, 1.7, 2.3, 1.6, 2.2, 2.0, 1.9,\n",
    "                   1.8, 1.7, 2.1, 2.4, 2.3, 1.9, 2.2, 1.8, 2.1, 1.7,\n",
    "                   2.0, 2.3, 1.6, 2.2, 2.1, 1.9, 2.0, 2.4, 1.7, 2.3,\n",
    "                   1.6, 2.2, 2.1, 1.9, 2.0, 2.3, 1.8, 2.2, 1.6, 2.1,\n",
    "                   2.4, 2.3, 1.7, 1.9, 2.0, 2.2, 1.8, 2.1, 2.3, 2.4])\n",
    "\n",
    "diet_c = np.array([0.9, 1.0, 0.8, 0.6, 0.7, 0.8, 1.1, 0.9, 0.7, 0.8,\n",
    "                   1.0, 0.6, 0.7, 0.9, 1.1, 0.8, 0.6, 1.0, 0.7, 0.9,\n",
    "                   0.8, 1.1, 0.9, 0.7, 0.8, 1.0, 0.6, 1.1, 0.9, 0.7,\n",
    "                   0.8, 1.0, 0.8, 0.6, 0.7, 0.9, 1.1, 0.8, 0.6, 1.0,\n",
    "                   0.9, 0.7, 0.8, 1.1, 0.9, 0.7, 0.8, 1.0, 0.6, 0.7])\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "f_statistic, p_value = stats.f_oneway(diet_a, diet_b, diet_c)\n",
    "\n",
    "print(\"F-Statistic:\", f_statistic)\n",
    "print(\"P-Value:\", p_value)\n",
    "\n",
    "# Interpret the results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"There are significant differences between the mean weight loss of the three diets.\")\n",
    "else:\n",
    "    print(\"There is no significant difference between the mean weight loss of the three diets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       sum_sq    df         F    PR(>F)\n",
      "Program              1.035327   2.0  0.136986  0.872659\n",
      "Experience           0.521940   1.0  0.138118  0.713420\n",
      "Program:Experience   2.683910   2.0  0.355113  0.704716\n",
      "Residual            90.694755  24.0       NaN       NaN\n"
     ]
    }
   ],
   "source": [
    "# Q10. A company wants to know if there are any significant differences in the average time it takes to\n",
    "# complete a task using three different software programs: Program A, Program B, and Program C. They\n",
    "# randomly assign 30 employees to one of the programs and record the time it takes each employee to\n",
    "# complete the task. Conduct a two-way ANOVA using Python to determine if there are any main effects or\n",
    "# interaction effects between the software programs and employee experience level (novice vs.\n",
    "# experienced). Report the F-statistics and p-values, and interpret the results.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Create example data\n",
    "np.random.seed(42)\n",
    "n = 30\n",
    "programs = np.random.choice(['A', 'B', 'C'], n)\n",
    "experience = np.random.choice(['novice', 'experienced'], n)\n",
    "times = np.random.normal(loc=10, scale=2, size=n)\n",
    "\n",
    "# Create a DataFrame\n",
    "data = pd.DataFrame({'Program': programs, 'Experience': experience, 'Time': times})\n",
    "\n",
    "# Perform two-way ANOVA\n",
    "model = ols('Time ~ Program * Experience', data=data).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "print(anova_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-Statistic: -6.156404161485698\n",
      "P-Value: 1.6369218868074588e-08\n",
      "There is a significant difference in test scores between the two groups.\n",
      " Multiple Comparison of Means - Tukey HSD, FWER=0.05  \n",
      "======================================================\n",
      " group1    group2    meandiff p-adj lower upper reject\n",
      "------------------------------------------------------\n",
      "Control Experimental     4.88   0.0 3.307 6.453   True\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Q11. An educational researcher is interested in whether a new teaching method improves student test\n",
    "# scores. They randomly assign 100 students to either the control group (traditional teaching method) or the\n",
    "# experimental group (new teaching method) and administer a test at the end of the semester. Conduct a\n",
    "# two-sample t-test using Python to determine if there are any significant differences in test scores\n",
    "# between the two groups. If the results are significant, follow up with a post-hoc test to determine which\n",
    "# group(s) differ significantly from each other.\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Example data for control and experimental groups\n",
    "control_scores = np.array([78, 82, 85, 75, 92, 88, 80, 84, 79, 87,\n",
    "                           90, 76, 82, 89, 83, 85, 81, 88, 86, 77,\n",
    "                           79, 83, 80, 82, 81, 85, 87, 88, 84, 79,\n",
    "                           81, 86, 90, 78, 79, 81, 82, 88, 85, 83,\n",
    "                           76, 87, 80, 82, 89, 84, 85, 86, 90, 82])\n",
    "\n",
    "experimental_scores = np.array([85, 88, 92, 79, 94, 89, 82, 87, 90, 91,\n",
    "                                95, 81, 87, 93, 89, 91, 86, 90, 88, 83,\n",
    "                                86, 90, 85, 87, 88, 91, 89, 90, 92, 85,\n",
    "                                88, 93, 95, 82, 84, 87, 89, 90, 88, 86,\n",
    "                                80, 92, 87, 89, 93, 88, 89, 90, 93, 87])\n",
    "\n",
    "# Perform two-sample t-test\n",
    "t_statistic, p_value = stats.ttest_ind(control_scores, experimental_scores)\n",
    "\n",
    "print(\"T-Statistic:\", t_statistic)\n",
    "print(\"P-Value:\", p_value)\n",
    "\n",
    "# Interpret the t-test results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"There is a significant difference in test scores between the two groups.\")\n",
    "else:\n",
    "    print(\"There is no significant difference in test scores between the two groups.\")\n",
    "\n",
    "# Perform post-hoc test (Tukey's HSD)\n",
    "if p_value < alpha:\n",
    "    all_scores = np.concatenate((control_scores, experimental_scores))\n",
    "    group_labels = ['Control'] * len(control_scores) + ['Experimental'] * len(experimental_scores)\n",
    "    posthoc = pairwise_tukeyhsd(all_scores, group_labels, alpha=0.05)\n",
    "    print(posthoc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-Statistic: 25.32276112538273\n",
      "P-Value: 2.1524678060485753e-09\n",
      "There are significant differences in daily sales between the three stores.\n"
     ]
    }
   ],
   "source": [
    "# Q12. A researcher wants to know if there are any significant differences in the average daily sales of three\n",
    "# retail stores: Store A, Store B, and Store C. They randomly select 30 days and record the sales for each store\n",
    "# on those days. Conduct a repeated measures ANOVA using Python to determine if there are any\n",
    "\n",
    "# significant differences in sales between the three stores. If the results are significant, follow up with a post-\n",
    "# hoc test to determine which store(s) differ significantly from each other.\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Example data for daily sales of three stores (A, B, C) over 30 days\n",
    "store_a_sales = np.random.normal(loc=1000, scale=100, size=30)\n",
    "store_b_sales = np.random.normal(loc=1100, scale=120, size=30)\n",
    "store_c_sales = np.random.normal(loc=900, scale=110, size=30)\n",
    "\n",
    "# Combine data from all stores\n",
    "all_sales = np.concatenate((store_a_sales, store_b_sales, store_c_sales))\n",
    "\n",
    "# Create corresponding labels for store identification\n",
    "store_labels = ['A'] * 30 + ['B'] * 30 + ['C'] * 30\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "f_statistic, p_value = stats.f_oneway(store_a_sales, store_b_sales, store_c_sales)\n",
    "\n",
    "print(\"F-Statistic:\", f_statistic)\n",
    "print(\"P-Value:\", p_value)\n",
    "\n",
    "# Interpret the results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"There are significant differences in daily sales between the three stores.\")\n",
    "else:\n",
    "    print(\"There is no significant difference in daily sales between the three stores.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
